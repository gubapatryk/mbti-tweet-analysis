{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv(\"mbti_1.csv\")\n",
    "dataset2 = pd.read_csv(\"mbti_full_pull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_personalities = pd.unique(dataset1['type']).tolist()\n",
    "valid_personalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_search = valid_personalities + [a for a in map(str.lower, valid_personalities)]\n",
    "to_search += [a for a in map(str.capitalize, valid_personalities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset2.loc[dataset2['subreddit'].isin(to_search)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df = df[['body', 'subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].apply(lambda x: str.upper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(df['subreddit']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df.groupby(['subreddit']).count()\n",
    "total\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "\n",
    "plt.bar(np.array(total.index), height = total['body'],)\n",
    "plt.xlabel('Personality types', size = 14)\n",
    "plt.ylabel('Number of posts available', size = 14)\n",
    "plt.title('Total posts for each personality type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=['type', 'post'])\n",
    "\n",
    "i=0\n",
    "for index, row in dataset1.iterrows():\n",
    "    split_row = dataset1.iloc[index,1].split('|||')\n",
    "    for post in split_row:\n",
    "        dictio = {'type' : row['type'], 'post': post}\n",
    "        df2 = df2.append(dictio, ignore_index = True)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"body\": \"post\", \"subreddit\": \"type\"})\n",
    "df = pd.concat([df, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(df, remove_special=True):\n",
    "    texts = df['post'].copy()\n",
    "    labels = df['type'].copy()\n",
    "\n",
    "    #Remove links \n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'https?:\\/\\/.*?[\\s+]', '', str(x).replace(\"|\",\" \") + \" \"))\n",
    "    \n",
    "    #Keep the End Of Sentence characters\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', str(x) + \" \"))\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', str(x) + \" \"))\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', str(x) + \" \"))\n",
    "    \n",
    "    #Strip Punctation\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'[\\.+]', \".\",str(x)))\n",
    "\n",
    "    #Remove multiple fullstops\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'[^\\w\\s]','',str(x)))\n",
    "\n",
    "    #Remove Non-words\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',str(x)))\n",
    "\n",
    "    #Convert posts to lowercase\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: str(x).lower())\n",
    "\n",
    "    #Remove multiple letter repeating words\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',str(x))) \n",
    "\n",
    "    #Remove very short or long words\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',str(x)))\n",
    "    df[\"post\"] = df[\"post\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',str(x)))\n",
    "\n",
    "    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n",
    "    if remove_special:\n",
    "        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n",
    "        pers_types = [p.lower() for p in pers_types]\n",
    "        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n",
    "        df['new'] = df['string'].str.replace(pat, '')\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Preprocessing of entered Text\n",
    "new_df = preprocess_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df.info())\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('mbti_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short = new_df.groupby('type').apply(lambda x: x.sample(n=1907)).reset_index(drop = True)\n",
    "short.to_csv('mbti_short.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
